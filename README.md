This repository contains the code and models for the following project: <br>
**Adapting to Context: A Case Study on In-Context Learning of Decision Tree Algorithms by Large Language Models** as a part of the final project for [INFO 259: Natural Language Processing](https://ucbnlp24.github.io/webpage/) at UC Berkeley 

Note: This repository and the pretrained model check-points for noise-free training are downloaded and built-up on from the following paper: What Can Transformers Learn In-Context? A Case Study of Simple Function Classes and this repository is forked from: (https://github.com/dtsip/in-context-learning)[https://github.com/dtsip/in-context-learning]


## Getting Started 
You can start by cloning the repository and following the steps below. 

1. Install the dependencies for the code using a Conda virtual environment. Note: You would need to adjust the environment YAML file depending on the setup, especially if performing model training in Google Colab

```python
conda env create -f environment.yml
conda activate in-context-learning
```

2. Download the 

